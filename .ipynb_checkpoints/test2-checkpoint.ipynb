{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c31b73a9-6d08-4f1f-bb86-7fed1e2a5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4232b8c-4458-48da-bc02-48a160459bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac7db2-d697-4be9-8d6c-2006252d5e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2680f61b-dae8-4ebc-9163-3c094bf4f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "def get_EF(input_size, dim, method=\"learnable\", head_dim=None, bias=True):\n",
    "    \"\"\"\n",
    "    Retuns the E or F matrix, initialized via xavier initialization.\n",
    "    This is the recommended way to do it according to the authors of the paper.\n",
    "    Includes a method for convolution, as well as a method for no additional params.\n",
    "    \"\"\"\n",
    "    assert method == \"learnable\" or method == \"convolution\" or method == \"no_params\", \"The method flag needs to be either 'learnable', 'convolution', or 'no_params'!\"\n",
    "    if method == \"convolution\":\n",
    "        conv = nn.Conv1d(head_dim, head_dim, kernel_size=int(input_size/dim), stride=int(input_size/dim))\n",
    "        return conv\n",
    "    if method == \"no_params\":\n",
    "        mat = torch.zeros((input_size, dim))\n",
    "        torch.nn.init.normal_(mat, mean=0.0, std=1/dim)\n",
    "        return mat\n",
    "    lin = nn.Linear(input_size, dim, bias)\n",
    "    torch.nn.init.xavier_normal_(lin.weight)\n",
    "    return lin\n",
    "\n",
    "class LinearAttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear attention, as proposed by the linformer paper\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, dropout, E_proj, F_proj, causal_mask, full_attention=False):\n",
    "        super(LinearAttentionHead, self).__init__()\n",
    "        self.E = E_proj\n",
    "        self.F = F_proj\n",
    "        self.dim = dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P_bar = None\n",
    "        self.full_attention = full_attention\n",
    "        self.causal_mask = causal_mask\n",
    "        self.is_proj_tensor = isinstance(E_proj, torch.Tensor)\n",
    "\n",
    "    def forward(self, Q, K, V, **kwargs):\n",
    "        \"\"\"\n",
    "        Assume Q, K, V have same dtype\n",
    "        E, F are `nn.Linear` modules\n",
    "        \"\"\"\n",
    "        input_mask = kwargs[\"input_mask\"] if \"input_mask\" in kwargs else None\n",
    "        embeddings_mask = kwargs[\"embeddings_mask\"] if \"embeddings_mask\" in kwargs else None\n",
    "\n",
    "        # Instead of classic masking, we have to do this, because the classic mask is of size nxn\n",
    "        if input_mask is not None:\n",
    "            # This is for k, v\n",
    "            mask = input_mask[:,:,None]\n",
    "            K = K.masked_fill_(~mask, 0.0)\n",
    "            V = V.masked_fill_(~mask, 0.0)\n",
    "            del mask\n",
    "\n",
    "        if embeddings_mask is not None:\n",
    "            mask = embeddings_mask[:,:,None]\n",
    "            Q = Q.masked_fill_(~mask, 0.0)\n",
    "            del mask\n",
    "\n",
    "        K = K.transpose(1,2)\n",
    "        if not self.full_attention:\n",
    "            if self.is_proj_tensor:\n",
    "                self.E = self.E.to(K.device)\n",
    "                K = torch.matmul(K, self.E)\n",
    "            else:\n",
    "                K = self.E(K)\n",
    "        Q = torch.matmul(Q, K)\n",
    "\n",
    "        P_bar = Q/torch.sqrt(torch.tensor(self.dim).type(Q.type())).to(Q.device)\n",
    "        if self.causal_mask is not None:\n",
    "            self.causal_mask = self.causal_mask.to(Q.device)\n",
    "            P_bar = P_bar.masked_fill_(~self.causal_mask, float('-inf'))\n",
    "        P_bar = P_bar.softmax(dim=-1)\n",
    "\n",
    "        # Only save this when visualizing\n",
    "        if \"visualize\" in kwargs and kwargs[\"visualize\"] == True:\n",
    "            self.P_bar = P_bar\n",
    "\n",
    "        P_bar = self.dropout(P_bar)\n",
    "\n",
    "        if not self.full_attention:\n",
    "            V = V.transpose(1,2)\n",
    "            if self.is_proj_tensor:\n",
    "                self.F = self.F.to(V.device)\n",
    "                V = torch.matmul(V, self.F)\n",
    "            else:\n",
    "                V = self.F(V)\n",
    "            V = V.transpose(1,2)\n",
    "        out_tensor = torch.matmul(P_bar, V)\n",
    "\n",
    "        return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0115868-35d2-43d8-9b86-f3920bda4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = get_EF(384, 100)\n",
    "F = get_EF(384, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61530dac-321c-4f02-884d-f76bfd1d3170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=384, out_features=100, bias=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "128519bc-5c6e-49d9-b3f7-8da63fbe3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearAttentionHead(\n",
    "        dim=64, # Dim 2 of the input\n",
    "        dropout=0.1, # Dropout of the P matrix\n",
    "        E_proj=E,\n",
    "        F_proj=F, # The E and F layers\n",
    "        causal_mask=None,\n",
    "        full_attention=False, # Use Full Attention instead\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3c66320-127e-4607-88c8-49d76fd136c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 384, 232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5992f9cb-88c6-4f50-a8ae-b13ee811cb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x,x,x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b7b6-2e29-4cc8-8f80-5a02166041ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d008014-b8fe-4db2-b94b-07fbaa142193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbc878-2635-4c5e-9ebf-e37a23c0d34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96731ffd-84ae-4c28-8fdf-a30615e0592d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ec8ba-a405-4d05-a67d-5b5d3704a6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167509eb-0a4d-4600-8f26-ce57bd95d99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c233371-5856-4883-86be-37f630db5182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325cacb-7497-4f86-9373-f61983f65338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547c414-7fc5-4a10-ad6a-ef9a0d9c008c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf4854-0704-4e10-97b5-008086932fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d689b-83f7-40bf-970e-0b889c420348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063d62a-d3ac-4d53-9ed2-a31f3bae0a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e7bc4-0c6c-4c9e-a52e-ee12812ff873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f2845-c4f4-4948-9791-bad64fbbdec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f028576-7571-43ce-86de-7bb6a70e87e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d696fae5-7c6b-422c-801c-c58c0f703ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, default_val):\n",
    "    return val if val is not None else default_val\n",
    "def init_(tensor):\n",
    "    dim = tensor.shape[-1]\n",
    "    std = 1 / math.sqrt(dim)\n",
    "    tensor.uniform_(-std, std)\n",
    "    return tensor\n",
    "\n",
    "class LinformerSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, seq_len, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert (dim % heads) == 0, 'dimension must be divisible by the number of heads'\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.k = k\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        dim_head = default(dim_head, dim // heads)\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim_head * heads, bias = False)\n",
    "\n",
    "        kv_dim = dim_head if one_kv_head else (dim_head * heads)\n",
    "        self.to_k = nn.Linear(dim, kv_dim, bias = False)\n",
    "        self.proj_k = nn.Parameter(init_(torch.zeros(seq_len, k)))\n",
    "\n",
    "        self.share_kv = share_kv\n",
    "        if not share_kv:\n",
    "            self.to_v = nn.Linear(dim, kv_dim, bias = False)\n",
    "            self.proj_v = nn.Parameter(init_(torch.zeros(seq_len, k)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.Linear(dim_head * heads, dim)\n",
    "\n",
    "    def forward(self, x, context = None, **kwargs):\n",
    "        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n",
    "\n",
    "        kv_len = n if context is None else context.shape[1]\n",
    "        assert kv_len == self.seq_len, f'the sequence length of the key / values must be {self.seq_len} - {kv_len} given'\n",
    "\n",
    "        queries = self.to_q(x)\n",
    "\n",
    "        proj_seq_len = lambda args: torch.einsum('bnd,nk->bkd', *args)\n",
    "\n",
    "        kv_input = x if context is None else context\n",
    "\n",
    "        keys = self.to_k(kv_input)\n",
    "        values = self.to_v(kv_input) if not self.share_kv else keys\n",
    "\n",
    "        kv_projs = (self.proj_k, self.proj_v if not self.share_kv else self.proj_k)\n",
    "\n",
    "        # project keys and values along the sequence length dimension to k\n",
    "\n",
    "        keys, values = map(proj_seq_len, zip((keys, values), kv_projs))\n",
    "\n",
    "        # merge head into batch for queries and key / values\n",
    "\n",
    "        queries = queries.reshape(b, n, h, -1).transpose(1, 2)\n",
    "\n",
    "        merge_key_values = lambda t: t.reshape(b, k, -1, d_h).transpose(1, 2).expand(-1, h, -1, -1)\n",
    "        keys, values = map(merge_key_values, (keys, values))\n",
    "\n",
    "        # attention\n",
    "\n",
    "        dots = torch.einsum('bhnd,bhkd->bhnk', queries, keys) * (d_h ** -0.5)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('bhnk,bhkd->bhnd', attn, values)\n",
    "\n",
    "        # split heads\n",
    "        out = out.transpose(1, 2).reshape(b, n, -1)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7153cf0-8192-4fb3-972e-90e0be3036ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the sequence length of the key / values must be 4096 - 925 given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m attn \u001b[38;5;241m=\u001b[39m LinformerSelfAttention(\n\u001b[1;32m      2\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      3\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     share_kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m925\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.conda/envs/msa/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mLinformerSelfAttention.forward\u001b[0;34m(self, x, context, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m b, n, d, d_h, h, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_head, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\n\u001b[1;32m     39\u001b[0m kv_len \u001b[38;5;241m=\u001b[39m n \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kv_len \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe sequence length of the key / values must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkv_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_q(x)\n\u001b[1;32m     44\u001b[0m proj_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m args: torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnd,nk->bkd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mAssertionError\u001b[0m: the sequence length of the key / values must be 4096 - 925 given"
     ]
    }
   ],
   "source": [
    "attn = LinformerSelfAttention(\n",
    "    dim = 512,\n",
    "    seq_len = 4096,\n",
    "    heads = 8,\n",
    "    k = 256,\n",
    "    one_kv_head = True,\n",
    "    share_kv = True\n",
    ")\n",
    "\n",
    "x = torch.randn(32, 925, 512)\n",
    "attn(x).shape # (1, 4096, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52764518-2454-4a62-87c6-ec879f6b6858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msa",
   "language": "python",
   "name": "msa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
